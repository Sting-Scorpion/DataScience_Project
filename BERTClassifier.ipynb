{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "X = df['text']\n",
    "Y = df['suicide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It makes me happy to think that I'd rather commit suicide than to live an unhappy lifeAt least that is something that many of the people who are not suicidal cannot do.\\n\\nI used to worry a lot about my future being worried about if I fuck things up and I create a bad ad unhappy life for me that I'd have to deal for the rest of my life, but now that I know that I can just end it all if things go bad, that makes me feel much better for some reason...\",\n",
       " \"My dad got the Corona Virus... Please pray for hi am ya'll ðŸ¥º\",\n",
       " 'the everlasting question why is this art!!!!!!1211111!!!!!',\n",
       " 'If someone ik finds my reddit acct i am saying someone catfished using my pictures ong Im a loser but i could at least try to hide it ðŸ¥´',\n",
       " \"I lost everything in span of a month.Hey guys, \\n\\numm...so I'm pretty shook up right now. \\n\\nLast month the woman who I thought I will marry left me five years into our relationship. \\n\\nI sort of dealt with it and did my best to move on...got my own place and started to rebuild my life. \\n\\nToday I was let go from work. This has been a massive kick to the chest for me. \\n\\nI'm in shock right now and the two events are kind of piling up on top of each other. \\n\\nMy home and professional lives have been destroyed. I do not know what to do. \\n\\nI feel like I'm in a fog right now..I'm...lost. I just do not know what to do. \\n\\n\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [sen for sen in X]\n",
    "\n",
    "sentences[: 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_labels = torch.tensor(Y.values)\n",
    "\n",
    "sen_labels[: 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='./pretrained/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# get pre-trained tokenizer model\n",
    "tokenizer = BertTokenizer.from_pretrained('./pretrained/bert-base-uncased')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  2009,  3084,  2033,  3407,  2000,  2228,  2008,  1045,  1005,\n",
      "         1040,  2738, 10797,  5920,  2084,  2000,  2444,  2019, 12511,  2166,\n",
      "         4017,  2560,  2008,  2003,  2242,  2008,  2116,  1997,  1996,  2111,\n",
      "         2040,  2024,  2025, 26094,  3685,  2079,  1012,  1045,  2109,  2000,\n",
      "         4737,  1037,  2843,  2055,  2026,  2925,  2108,  5191,  2055,  2065,\n",
      "         1045,  6616,  2477,  2039,  1998,  1045,  3443,  1037,  2919,  4748,\n",
      "        12511,  2166,  2005,  2033,  2008,  1045,  1005,  1040,  2031,  2000,\n",
      "         3066,  2005,  1996,  2717,  1997,  2026,  2166,  1010,  2021,  2085,\n",
      "         2008,  1045,  2113,  2008,  1045,  2064,  2074,  2203,  2009,  2035,\n",
      "         2065,  2477,  2175,  2919,  1010,  2008,  3084,  2033,  2514,  2172,\n",
      "         2488,  2005,  2070,  3114,  1012,  1012,  1012,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "\n",
    "tokenized = tokenizer(sentences, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "print(tokenized['input_ids'][0])\n",
    "print(tokenized['token_type_ids'][0])\n",
    "print(tokenized['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([185659, 512])\n",
      "torch.Size([185659, 512])\n",
      "torch.Size([185659, 512])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized['input_ids'].size())\n",
    "print(tokenized['token_type_ids'].size())\n",
    "print(tokenized['attention_mask'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_ids = tokenized['input_ids']\n",
    "attention_mask = tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or using code below to do steps separately\n",
    "\n",
    "``` python\n",
    "# tokenize\n",
    "sentences = [\"[CLS] \" + sen + \" [SEP]\" for sen in sentences]\n",
    "sen_tokens = [tokenizer.tokenize(sen) for sen in sentences]\n",
    "print(sen_tokens[0])\n",
    "\n",
    "# transform tokens to ids\n",
    "sen_ids = [tokenizer.convert_tokens_to_ids(sen) for sen in sen_tokens]\n",
    "print(sen_ids[0])\n",
    "\n",
    "# padding\n",
    "max_length = 200\n",
    "sen_ids = pad_sequences(sen_ids, maxlen=max_length, dtype='long', truncating='post', padding='post')\n",
    "print(sen_ids[0])\n",
    "\n",
    "# create attention mask\n",
    "attention_mask = [[1 if id > 0 else 0 for id in sen] for sen in sen_ids]\n",
    "print(attention_mask[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer\n",
    "del tokenized\n",
    "del sentences\n",
    "del X\n",
    "del Y\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7446"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "A stratified 5-fold cross validation will be applied to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "def model_train(model, train_dataloader, optimizer, scheduler, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = []\n",
    "\n",
    "        model.train()\n",
    "        for i, batch_data in enumerate(train_dataloader):\n",
    "            input_ids, input_masks, input_labels = tuple(data.to(device) for data in batch_data)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output =  model(input_ids, attention_mask=input_masks, labels=input_labels, token_type_ids=None)\n",
    "\n",
    "            loss = output['loss']\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # train_loss.append(np.mean(train_losses))\n",
    "\n",
    "        print(\"Epoch: {}/{}\".format((epoch + 1), epochs),\n",
    "              \"\\n\\tTraining Loss: {:.4f}\".format(np.mean(train_losses)))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# validation model\n",
    "def model_validation(model, val_dataloader):\n",
    "    val_losses = []\n",
    "    accuracy_score_list, recall_score_list, precision_score_list, f1_score_list = [], [], [], []\n",
    "\n",
    "    model.eval()\n",
    "    for batch_data in val_dataloader:\n",
    "        input_ids, input_masks, input_labels = tuple(data.to(device) for data in batch_data)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(input_ids, attention_mask=input_masks, labels=input_labels, token_type_ids=None)\n",
    "\n",
    "        loss = preds['loss']\n",
    "        val_losses.append(loss.item())\n",
    "        preds = preds['logits'].detach().to('cpu').numpy()\n",
    "        labels = input_labels.to('cpu').numpy()\n",
    "\n",
    "        preds = preds.argmax(1).flatten() # shape = [1, : ]\n",
    "        labels = labels.flatten()\n",
    "\n",
    "        # Evaluate model\n",
    "        AccuracyScore = accuracy_score(labels, preds)\n",
    "        RecallScore = recall_score(labels, preds)\n",
    "        PrecisionScore = precision_score(labels, preds)\n",
    "        F1Score = f1_score(labels, preds)\n",
    "        \n",
    "        # Add to lists\n",
    "        accuracy_score_list.append(AccuracyScore)\n",
    "        recall_score_list.append(RecallScore)\n",
    "        precision_score_list.append(PrecisionScore)\n",
    "        f1_score_list.append(F1Score)\n",
    "\n",
    "    # val_loss.append(np.mean(val_losses))\n",
    "\n",
    "    print(\"Validation Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "          \"Validation Accuracy: {:.4f}%\".format(np.mean(accuracy_score_list) * 100))\n",
    "    \n",
    "    return np.mean(accuracy_score_list), np.mean(recall_score_list), np.mean(precision_score_list), np.mean(f1_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrained/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4 \n",
      "\tTraining Loss: 0.0916\n",
      "Epoch: 2/4 \n",
      "\tTraining Loss: 0.0426\n",
      "Epoch: 3/4 \n",
      "\tTraining Loss: 0.0169\n",
      "Epoch: 4/4 \n",
      "\tTraining Loss: 0.0043\n",
      "Validation Loss: 0.1037 Validation Accuracy: 97.9140%\n",
      "Time:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrained/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4 \n",
      "\tTraining Loss: 0.0913\n",
      "Epoch: 2/4 \n",
      "\tTraining Loss: 0.0418\n",
      "Epoch: 3/4 \n",
      "\tTraining Loss: 0.0169\n",
      "Epoch: 4/4 \n",
      "\tTraining Loss: 0.0045\n",
      "Validation Loss: 0.1112 Validation Accuracy: 97.7040%\n",
      "Time:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrained/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4 \n",
      "\tTraining Loss: 0.0920\n",
      "Epoch: 2/4 \n",
      "\tTraining Loss: 0.0431\n",
      "Epoch: 3/4 \n",
      "\tTraining Loss: 0.0179\n",
      "Epoch: 4/4 \n",
      "\tTraining Loss: 0.0055\n",
      "Validation Loss: 0.0989 Validation Accuracy: 97.7982%\n",
      "Time:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrained/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4 \n",
      "\tTraining Loss: 0.0899\n",
      "Epoch: 2/4 \n",
      "\tTraining Loss: 0.0408\n",
      "Epoch: 3/4 \n",
      "\tTraining Loss: 0.0157\n",
      "Epoch: 4/4 \n",
      "\tTraining Loss: 0.0045\n",
      "Validation Loss: 0.1025 Validation Accuracy: 97.9355%\n",
      "Time:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrained/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4 \n",
      "\tTraining Loss: 0.0898\n",
      "Epoch: 2/4 \n",
      "\tTraining Loss: 0.0418\n",
      "Epoch: 3/4 \n",
      "\tTraining Loss: 0.0168\n",
      "Epoch: 4/4 \n",
      "\tTraining Loss: 0.0049\n",
      "Validation Loss: 0.1038 Validation Accuracy: 97.8117%\n",
      "Accuracy: 97.83%\n",
      "Recall: 97.77%\n",
      "Precision: 97.89%\n",
      "F1_score: 97.76%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=60)\n",
    "accuracy_score_lists, recall_score_lists, precision_score_lists, f1_score_lists = [], [], [], []\n",
    "\n",
    "epochs = 4\n",
    "learning_rate = 5e-5\n",
    "epsilon = 1e-8\n",
    "\n",
    "for time, (train_index, val_index) in enumerate(skfolds.split(sen_ids, sen_labels)):\n",
    "    print('Time: ', time + 1)\n",
    "    X_train, X_val = sen_ids[train_index], sen_ids[val_index]\n",
    "    Y_train, Y_val = sen_labels[train_index], sen_labels[val_index]\n",
    "    mask_train, mask_val = attention_mask[train_index], attention_mask[val_index]\n",
    "\n",
    "    # pack dataloaders\n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, mask_train, Y_train)\n",
    "    val_dataset = TensorDataset(X_val, mask_val, Y_val)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained('./pretrained/bert-base-uncased', num_labels=2)\n",
    "    model = model.cuda()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, \n",
    "                                                num_training_steps=len(train_dataloader) * epochs)\n",
    "\n",
    "    # train model\n",
    "    model = model_train(model, train_dataloader, optimizer, scheduler, epochs)\n",
    "\n",
    "    # validate model\n",
    "    AccuracyScore, RecallScore, PrecisionScore, F1Score = model_validation(model, val_dataloader)\n",
    "\n",
    "    accuracy_score_lists.append(AccuracyScore)\n",
    "    recall_score_lists.append(RecallScore)\n",
    "    precision_score_lists.append(PrecisionScore)\n",
    "    f1_score_lists.append(F1Score)\n",
    "\n",
    "print(\"Accuracy: {:.2%}\".format(np.average(accuracy_score_lists)))\n",
    "print(\"Recall: {:.2%}\".format(np.average(recall_score_lists)))\n",
    "print(\"Precision: {:.2%}\".format(np.average(precision_score_lists)))\n",
    "print(\"F1_score: {:.2%}\".format(np.average(f1_score_lists)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with all data and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pretrained/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4 \n",
      "\tTraining Loss: 0.0886\n",
      "Epoch: 2/4 \n",
      "\tTraining Loss: 0.0420\n",
      "Epoch: 3/4 \n",
      "\tTraining Loss: 0.0182\n",
      "Epoch: 4/4 \n",
      "\tTraining Loss: 0.0056\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./model does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m model_train(model, train_dataloader, optimizer, scheduler, epochs)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./model/Bert_classifier.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ./model does not exist."
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(sen_ids, attention_mask, sen_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('./pretrained/bert-base-uncased', num_labels=2)\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, \n",
    "                                                num_training_steps=len(train_dataloader) * epochs)\n",
    "\n",
    "# train model\n",
    "model = model_train(model, train_dataloader, optimizer, scheduler, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'state_dict': model.state_dict(),\n",
    "    'config': model.config\n",
    "}, './model/Bert_classifier.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using code below to load the model:\n",
    "\n",
    "``` python\n",
    "checkpoint = torch.load('./model/Bert_classifier.pth')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('./pretrained/bert-base-uncased', config=checkpoint['config'])\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
