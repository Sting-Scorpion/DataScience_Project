% The document class supplies options to control rendering of some standard
% features in the result.  The goal is for uniform style, so some attention 
% to detail is *vital* with all fields.  Each field (i.e., text inside the
% curly braces below, so the MEng text inside {MEng} for instance) should 
% take into account the following:
%
% - author name       should be formatted as "FirstName LastName"
%   (not "Initial LastName" for example),
% - supervisor name   should be formatted as "Title FirstName LastName"
%   (where Title is "Dr." or "Prof." for example),
% - degree programme  should be "BSc", "MEng", "MSci", "MSc" or "PhD",
% - dissertation title should be correctly capitalised (plus you can have
%   an optional sub-title if appropriate, or leave this field blank),
% - dissertation type should be formatted as one of the following:
%   * for the MEng degree programme either "enterprise" or "research" to
%     reflect the stream,
%   * for the MSc  degree programme "$X/Y/Z$" for a project deemed to be
%     X%, Y% and Z% of type I, II and III.
% - year              should be formatted as a 4-digit year of submission
%   (so 2014 rather than the academic year, say 2013/14 say).

\documentclass[ % the name of the author
                    author={Louis Wang},
                % the name of the supervisor
                supervisor={Dr. Qiang Liu},
                % the degree programme
                    degree={MSc},
                % the dissertation    title (which cannot be blank)
                     title={Identification of Suicide Ideation in Texts},
                % the dissertation subtitle (which can    be blank)
                  %subtitle={And those including an optional subtitle too, for good measure},
                % the dissertation     type
                      type={},
                % the year of submission
                      year={2024}]{dissertation}

\begin{document}

% =============================================================================

% This macro creates the standard UoB title page by using information drawn
% from the document class (meaning it is vital you select the correct degree 
% title and so on).


\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter

% This macro creates the standard UoB declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makedecl

% LaTeX automatically generates a table of contents, plus associated lists 
% of figures, tables and algorithms.  The former is a compulsory part of the
% dissertation, but if you do not require the latter they can be suppressed
% by simply commenting out the associated macro.

\tableofcontents
\listoffigures
\listoftables
\listofalgorithms
\lstlistoflistings

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Abstract}


% -----------------------------------------------------------------------------

\chapter*{Supporting Technologies}

\noindent
This section lists the various technologies that I have used to implement and submit my dissertation project.

\begin{itemize}

\item Python was the main programming language used in this project. Code was developed with the version of Python 3.9. 
The following Python packages were used in the project:

\begin{itemize}
      \item {\em Scikit-Learn} for machine learning model and matric calculation
      \item {\em Pytorch} was used as the framework of deep learning model and for building, training and testing the neural networks
      \item {\em Transformer} for its implementation of BERT and other models related to it
      \item {\em Shap} for model explanation
      \item {\em Flask} was used as the web application framework 
\end{itemize}

\item AutoDL was used for online GPU resources and remote execution of code in jupyter notebook format
 
\item \LaTeX\ was used to format the thesis, via {\em Visual Studio Code} with LaTeX Workshop. 

\item GitHub was used to store, backup and share the source code, scripts, data, etc. of this project
\url{https://github.com/Sting-Scorpion/DataScience_Project}

\end{itemize}

% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

\noindent
The following list of notations and acronyms will be referenced in this project:

\begin{quote}
\noindent
\begin{tabular}{lcl}
      NLP &: & Natural Language Processing \\
      LR &: & Logistic Regression \\
      HMM &: & Hidden Markov Methods \\
      SVM &: & Support Vector Machines \\
      KNN &: & K Nearest Neighbours \\
      CRF &: & Conditional Random Fields \\
      BoW &: & Bag-of-Words \\
      CNN &: & Convolutional Neural Network \\
      RNN &: & Recurrent Neural Networks \\
      LSTM &: & Long Short-Term Memory \\
      BiLSTM &: & Bidirectional Long Short-Term Memory \\
      BERT &: & Bidirectional Encoder Representations from Transformers \\
      GPU &: & Graphics Processing Unit \\
      ${\mathcal H}( x )$ &: & the Hamming weight of $x$ \\
\end{tabular}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Acknowledgements}

\noindent

% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.

\mainmatter

% -----------------------------------------------------------------------------

\chapter{Introduction}
\label{chap:introduction}

% putting a \noindent before the first para in each chapter looks nicer.

\section{Suicide Ideation Identification Overview}

\section{Statement of Suicide Ideation Identification}

\section{Purpose of the Project}

\section{Structure}


% -----------------------------------------------------------------------------

\chapter{Related Work}
\label{chap:background}

\noindent
Currently natural language processing technology is becoming increasingly sophisticated, and social networks play an important role in people's daily lives, with people expressing their inner thoughts through social networks. 
Evidence suggests that introverts are highly inclined to engage in online communication, which leads to more self-expression. Signals of suicidal ideation and intentions are increasingly appearing on social networks, prompting significant interest among scholars in computer science and psychology in conducting mental health research through internet or social network-based platforms.

\section{Suicidal Ideation in Texts}
\noindent
In some countries, the traditional psychological approach of using a form or face-to-face interview to assess suicidal ideation is still largely used when studying whether a person is suicidal. However, this method has a high degree of volatility, as often people with suicidal attempts do not actively seek help, and it is difficult to prevent subjects from deliberately hiding their true inner thoughts. This method is difficult to use as an early warning.

\section{Conventional Machine Learning}
\noindent
Machine learning can be categorized into two main types: conventional machine learning and deep learning. Conventional machine learning starts from observation samples and tries to discover the complex laws behind them to achieve the prediction of future data trends. One of the important theoretical foundations of conventional machine learning algorithms is statistics, which has gained wide application in many computer fields such as natural language processing, speech recognition and image recognition. Related algorithms include Logistic Regression (LR), Hidden Markov Methods (HMM), Support Vector Machines (SVM), K Nearest Neighbours (KNN), Bayesian Methods, Decision Trees etc., mainly utilized for classification and regression tasks in the case of limited samples.

The main process of sentiment classification based on traditional machine learning is as follows. Firstly, the collected text data are labelled and processed to form a training set. Subsequently, the processed data undergo feature extraction. Following this, an appropriate supervised learning model is selected for training. Finally the trained model is able to predict the sentiment of unseen, new samples.

Pang et al. in 2002 first proposed the use of standard machine learning methods in solving emotion classification problems, with Naive Bayes, Maximum Entropy and SVM. And the experimental results show that SVM has the best effect among these methods. But these methods do not perform as well as on customary subject related classification.\cite{pang2002thumbs} B. Aliman et al. used several different models for binary sentiment analysis of tweets. The results show that Logistic Regression model outperforms SVM and Naive Bayes for the prediction of potential mental health crisis. \cite{aliman2022sentiment}

Despite the fact that conventional machine learning methods have achieved successes on some datasets and their ability to reduce labor costs. However, there are still some limitations in feature extraction, which makes it difficult to effectively extract features and mine deep semantics.

\section{Deep Learning}
\noindent
In recent years, with advancements in computer hardware performance, deep learning was first introduced by Hinton et al.\cite{hinton2006reducing} as a soluation to the shortcomings of conventional machine learning methods. The core of deep learning is its backpropagation algorithm. Unlike conventional machine learning methods, deep learning methods can efficiently extract deeper semantics from text, resulting in significant improvements in performance. Thus deep learning approaches have dominated the field of natural language processing.

Currently the mainstream deep learning neural networks are: Convolutional Neural Networks(CNN)\cite{chua1998cnn} and Recurrent Neural Networks (RNN)\cite{socher2011parsing}. CNN excels in extracting static features from text and capturing regional information. KIM firstly introduced CNN into text processing tasks. With pre-trained word vectors from Google News, significant advancements were achieved in sentence-level text sentiment classification.\cite{2014Convolutional} 

However, CNN mainly focuses on sequence features in the text space, and cannot effectively capture dependencies within time-series data. Therefore, RNN, which is mainly designed for processing time-series data, is introduced to capture the dependencies between words, thereby facilitating the extraction of global information of text. RNN consists of multiple neurons that compute sequentially, where each neuron's input contains the output of the previous neuron, which gives the network a memory capability and the ability to handle variable length sequences.

\subsection{LSTM}
\noindent
However, when dealing with excessively long sequences, the RNN may suffer from gradient vanishing or gradient explosion, thus failing to solve the problem of long-range dependency. Hochreiter et al. improved on the RNN and proposed Long Short-Term Memory (LSTM).\cite{hochreiter1997long} Forget gates are used to regulate the retention and discarding of information\cite{greff2016lstm}, mitigating the limitations of traditional RNN and enhancing the capacity to capture long-range dependency. Tang et al. applied the LSTM to aspect-level sentiment analysis.\cite{tang2015target}

Traditional LSTM can only utilize the information of previous moments to predict current moments' information, but information from current moment may also be related to a future moment. To address this problem, bidirectional LSTM (BiLSTM) model was proposed, combining the outputs of the lstm units in both forward and backward directions and also leveraging contextual information. Experiments have demonstrated that BiLSTM models are usually more effective in dealing with contextual information compared to LSTM models that are unidirectional.\cite{hameed2019computationally}

\subsection{BERT}
\noindent
Bidirectional Encoder Representations from Transformers(BERT) was first proposed by Devlin et al. on the Google AI Language team.\cite{devlin2018bert}, represents a groundbreaking language representation model. Unlike traditional unidirectional language models, BERT aims to pre-train deep bidirectional representations by considering both left and right contexts. This model ushers a new era of large-scale pre-training based language models that significantly advances the field of natural language processing.

BERT mainly consists of multiple Transformers encoders, and each layer contains self-attention mechanism and feed-forward neural network.\cite{vaswani2017attention} By pre-training on extensive datasets and fine-tuning the BERT model, deep bi-directional linguistic representations are generated, which can be used in different NLP downstream tasks. Research by Bilal et al. employed BERT for sentiment classification\cite{bilal2023effectiveness}, Qu et al. utilized BERT for question-answering tasks\cite{qu2019bert}, and Miller et al. applied BERT for summarising texts\cite{miller2019leveraging}. The powerful advantages of pre-trained models have led increasing scholarly interest in NLP research with BERT.

Furthermore, scholars have introduced many variant models based on BERT to target improvements in diverse tasks and scenarios. RoBERTa, proposed by Facebook AI in 2019\cite{liu2019roberta}, improves the performance of the model with a larger vocabulary list, extended training sequences, augmented data, and diverse pre-training tasks. DistillBERT, introduced by Hugging Face in 2019\cite{sanh2020distilbert}, is a simplified version of BERT that approximates the performance of BERT with a smaller, faster model by distilling key information from BERT through knowledge distillation techniques. Additionally, BART, proposed by Facebook AI in 2019\cite{lewis2019bart}, adopts the BERT architecture for sequence-to-sequence tasks such as text summarisation and text generation.

\section{Explainable AI}

% -----------------------------------------------------------------------------

\chapter{Baseline Model}
\label{chap:execution1}

\noindent
This chapter will cover an approach to sentiment classification based on baseline model. The initial section ooutlines how to pre-process the data, and transform textual data into a format compatible with the model. Subsequently, the second section contains an explanation of the baseline model. Moving forward, the third section tries to enhance the model performance by optimising the data processing part based through the utilization of word embedding. Following this, the fourth section evaluates the model through the analysis of the performance metrics. Finally, the last section includes the explanation of the model, dissecting the factors contributing to both its good and bad performances.

\section{Preprocess Data}
\noindent
Since algorithms can only handle numeric data, data preprocessing is essential to enable text data to be received by the model and facilitate learning features. Raw text often contains a lot of redundant and repetitive information, while data preprocessing can help the algorithm to focus on the core content of the text, which improves the accuracy of the algorithm.

\subsection{Exploratory Data Analysis}
\noindent
Firstly, the entire dataset undergoes the test to see whether it is balanced or not, and if there are any missing values. Alongside the length distribution within each category is analyzed. Upon dataset examination, it's revealed that the label consists of two categories: "suicide" and "non-suicide". The sample size of both categories is 116,037 entries, which is a balanced dataset with no missing values. However, there's significant variance in the average data length between categories. Figure \ref{fig:describe} depicts the relationship between sample length, in terms of the number of tokens, and sample numbers. 

\begin{figure}[h]
      \centering
      \includegraphics[width=0.6\linewidth]{../img/data_describe.png}
      \caption{Distribution of data across samples of varying lengths}
      \label{fig:describe}
\end{figure}

Notably, the average sample length in the "suicide" category exceeds that of the "non-suicide" category. The specific data is shown in the Table \ref{tab:describe}.

\begin{table}[h]
      \centering
      \begin{tabular}{ccccc}
            \hline
            class & count & mean length & min & max \\
            \hline
            suicide & 116037 & 61.188 & 2 & 8220 \\
            non-suicide & 116037 & 202.662 & 1 & 9684 \\
            \hline
      \end{tabular}
      \caption{Data Description}
      \label{tab:describe}
\end{table}

Following the initial analysis, word clouds were generated for each of the two categories of data. A word cloud offers a visual depiction of word frequency. The more frequently a word occurs within the text being analysed, the larger that word is in the generated image. This visualization technique is increasingly being used as a simple tool for determining the focus of written material.\cite{atenstaedt2012word} Figure \ref{fig:word cloud} shows the word cloud of both "non-suicide"(Figure \ref{fig:word cloud}\ref{sub@wc_n}) and "suicide"(Figure \ref{fig:word cloud}\ref{sub@wc_s}) categories.

\begin{figure}[h]
      \centering
      \subfloat["non-suicide" Class]{
            \includegraphics[width=0.44\linewidth]{../img/wc_nonsuicide.jpg}
            \label{wc_n}}
      \hfil
      \subfloat["suicide" Class]{
            \includegraphics[width=0.44\linewidth]{../img/wc_suicide.jpg}
            \label{wc_s}}
      \caption{Word Cloud of both two Categories of Raw Texts}
      \label{fig:word cloud}
\end{figure}

\subsection{Data Cleaning}
\noindent
After performing exploratory data analysis, a general understanding of the data set has been obtained. The next step should be to preprocess the data.

Missing values can appear due to various reasons, such as data entry errors, sensor failures, or simply due to the nature of the data collection process. Common processing methods to deal with missing values include deletion, filling with special values, marking as a new class, and so on. The section of a method depends on the characteristics of the dataset and the most appropriate method must be chosen accordingly. However, since there are no missing values in this dataset, this step is not required.

Given the presence of numerous abbreviations in the text, which may lead to interpretational confusion. Therefore it is essential to convert common abbreviations into their full forms. This can be handled with a dictionary structure in Python, containing common abbreviations and corresponding full forms. Each word in the text can then be iterated through regular expressions and the abbreviations contained in the dictionary are replaced.

What's more, the dataset may contain many unwanted content such as URLs, emojis, special symbols, etc. These are not conductive to semantic analysis of the text and may even interfere with the learning process of the model. Such content can be filtered out with regular expressions.

Finally, it is vital to remove the stopwords in the text. Stopwords are words that appear frequently in the text (e.g., "the", "is", "and") but contribute minimally to the meaning of the text. Removing these words can enhance text processing efficiency. Many organisations published their pre-made stopwords lists. Here the English stopword list provided by the NLTK library is used to remove stopwords from the dataset.

Subsequently, Figure \ref{fig:word cloud clean} shows the word cloud of the dataset extracted after data cleaning to compare the result before and after data cleaning.

\begin{figure}[h]
      \centering
      \subfloat["non-suicide" Class after Cleaning]{
            \includegraphics[width=0.44\linewidth]{../img/wc_nonsuicide_c.jpg}
            \label{wc_n_c}}
      \hfil
      \subfloat["suicide" Class after Cleaning]{
            \includegraphics[width=0.44\linewidth]{../img/wc_suicide_c.jpg}
            \label{wc_s_c}}
      \caption{Word Cloud of both two Categories after Data Cleaning}
      \label{fig:word cloud clean}
\end{figure}

\subsection{Word Vectorization}
\noindent
After cleaning the dataset, the text needs to be vectorially represented. Bag-of-words (BoW) model is one of the commonly used vector representation models among so many models. BoW model, based on one-hot encoding\cite{chren1998one}, is a simple yet powerful technique used in NLP for representing text data. It's based on the concept of treating text as a "bag" of individual words, disregarding grammar and word order, and focusing solely on word frequency. The workflow of the model is divided into the following steps:

\begin{enumerate}
      \item Tokenization: The first step is to break down the text into individual tokens. This process involves splitting the text into words based on whitespace characters.
      \item Vocabulary Creation: Next, a vocabulary is created by compiling a list of unique words present in the entire dataset. Each unique word in the vocabulary becomes a feature in the BoW model.
      \item Vectorization: For each document in the dataset, a vector is constructed where each element represents the frequency of a word from the vocabulary in that document. The length of the vector is equal to the size of the vocabulary, and the values are the counts of each words in a document.
      \item Sparse Representation: Since most documents only contain a small subset of the words in the vocabulary, the resulting vectors are typically sparse, meaning that most of the elements are zero.
\end{enumerate}

During implementation, the maximum number of features can be set to prevent excessive dimensionality and to speed up processing. Finally a sparse matrix representing the number of occurrences of the word will be obtained as the word vector passed to the model.

\section{Logistic Regression}
\noindent
After the data pre-processing step, a vector-like representation of the data is obtained, which can be served as the training and testing data for the baseline model. Here, logistic regression is used as the baseline model. Logistic regression is a classification method in the field of statistics and machine learning. Despite its name, logistic regression is a classification algorithm, not a regression algorithm. The goal of logistic regression is to predict the probability of an event occurring, achieved through a combination of linear regression and an activation function.

As a generalized linear regression algorithm, logistic regression initially employs a linear regression algorithm to model the relationship between the independent (features) and dependent (outcome) variables. The linear equation takes the following form:
\begin{eqnarray}
      z=b_0+b_1x_1+b_2x_2+\cdots+b_nx_n
\end{eqnarray}

Where $z$ represents the log-odds of the probability of the positive class, $x_1,x_2,\cdots,x_n$ are the feature variables, and $b_1,b_2,\cdots,b_n$ are the coefficients to be estimated. The linear result $z$ is next passed to a logistic function that converts the log odds into a probability ranging between 0 and 1. The sigmoid function is most commonly used for this transformation, defined as:
\begin{eqnarray}
      p(z)=\frac{1}{1 + e^{-z}}
\end{eqnarray}

Where $p$ represents the probability of a positive prediction. The complete logistic regression formula can be obtained by combining the two formulas:
\begin{eqnarray}
      p=\frac{1}{1 + e^{-(b_0+b_1x_1+b_2x_2+\cdots+b_nx_n)}}
\end{eqnarray}

After that, a threshold is determined to serve as a dividing line (usually 0.5), making the predicted probability converted into a binary outcome. If the predicted probability is greater than the threshold, the instance is classified as belonging to the positive class; otherwise, it is classified as belonging to the negative class.

Logistic regression iteratively updates the coefficients $b_1,b_2,\cdots,b_n$ through optimisation algorithms such as maximum likelihood estimation (MSE) or stochastic gradient descent (SGD). The objective is to adjust the coefficients in such a way that the probability of positive category increased and the probability of negative category decreased. This iterative optimization process continues until convergence, where the model parameters converge to their optimal values, leading to the best model performance.

\section{Improve with Word Embedding}

\section{Baseline Performance}

\subsection{Logistic Regression with CountVectorizer}

\subsection{Logistic Regression with BERT Embedding}

\section{Baseline Model Explanation}

% -----------------------------------------------------------------------------

\chapter{Deep Learning Model}
\label{chap:execution2}

\section{Preprocess Data}

\section{Recurrent Neural Network}

\subsection{Bi-LSTM Model}

\subsection{Bi-LSTM Model Performance}

\subsection{Bi-LSTM Model Explanation}

\section{Large Language Model}

\subsection{BERTForSequenceClassification}

\subsection{BERT Model Performance}

\subsection{BERT Model Explanation}

% -----------------------------------------------------------------------------

\chapter{Suicide Ideation Identification System Implementation}
\label{chap:implementation}

% -----------------------------------------------------------------------------

\chapter{Conclusion}
\label{chap:conclusion}

\section{Summary}

\noindent

\section{Project Status}

\noindent
The aims of the project are as follows:

\begin{itemize}
      \item one
      \item two
\end{itemize}

\section{Further Work}

\noindent

\textbf{one}

\textbf{two}


% =============================================================================

% Finally, after the main matter, the back matter is specified.  This is
% typically populated with just the bibliography.  LaTeX deals with these
% in one of two ways, namely
%
% - inline, which roughly means the author specifies entries using the 
%   \bibitem macro and typesets them manually, or
% - using BiBTeX, which means entries are contained in a separate file
%   (which is essentially a database) then imported; this is the 
%   approach used below, with the databased being dissertation.bib.
%
% Either way, the each entry has a key (or identifier) which can be used
% in the main matter to cite it, e.g., \cite{X}, \cite[Chapter 2}{Y}.

\backmatter

\bibliography{sample_bibtex.bib}

% -----------------------------------------------------------------------------

% The dissertation concludes with a set of (optional) appendices; these are 
% the same as chapters in a sense, but once signalled as being appendices via
% the associated macro, LaTeX manages them appropriately.

\appendix

\chapter{Appendix}
\label{appx:example}

Content which is not central to, but may enhance the dissertation can be 
included in one or more appendices; examples include, but are not limited
to

\begin{itemize}
\item lengthy mathematical proofs, numerical or graphical results which 
      are summarised in the main body,
\item sample or example calculations, 
      and
\item results of user studies or questionnaires.
\end{itemize}

\noindent
Note that in line with most research conferences, the examiners are not
obliged to read such appendices.

% =============================================================================

\end{document}
