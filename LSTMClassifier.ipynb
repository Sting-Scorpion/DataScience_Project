{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "X = df['text']\n",
    "Y = df['suicide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It makes me happy to think that I'd rather commit suicide than to live an unhappy lifeAt least that is something that many of the people who are not suicidal cannot do.\\n\\nI used to worry a lot about my future being worried about if I fuck things up and I create a bad ad unhappy life for me that I'd have to deal for the rest of my life, but now that I know that I can just end it all if things go bad, that makes me feel much better for some reason...\",\n",
       " \"My dad got the Corona Virus... Please pray for hi am ya'll ðŸ¥º\",\n",
       " 'the everlasting question why is this art!!!!!!1211111!!!!!',\n",
       " 'If someone ik finds my reddit acct i am saying someone catfished using my pictures ong Im a loser but i could at least try to hide it ðŸ¥´',\n",
       " \"I lost everything in span of a month.Hey guys, \\n\\numm...so I'm pretty shook up right now. \\n\\nLast month the woman who I thought I will marry left me five years into our relationship. \\n\\nI sort of dealt with it and did my best to move on...got my own place and started to rebuild my life. \\n\\nToday I was let go from work. This has been a massive kick to the chest for me. \\n\\nI'm in shock right now and the two events are kind of piling up on top of each other. \\n\\nMy home and professional lives have been destroyed. I do not know what to do. \\n\\nI feel like I'm in a fog right now..I'm...lost. I just do not know what to do. \\n\\n\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [sen for sen in X]\n",
    "\n",
    "sentences[: 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_labels = torch.tensor(Y.values)\n",
    "\n",
    "sen_labels[: 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='./pretrained/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# get pre-trained tokenizer model\n",
    "tokenizer = BertTokenizer.from_pretrained('./pretrained/bert-base-uncased')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "tokenized = tokenizer(sentences, padding=True, truncation=True, max_length=max_length, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([185659, 512])\n"
     ]
    }
   ],
   "source": [
    "sen_ids = tokenized['input_ids']\n",
    "\n",
    "print(sen_ids.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Releasing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tokenizer\n",
    "del tokenized\n",
    "del sentences\n",
    "del X\n",
    "del Y\n",
    "del df\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "```\n",
    "LSTMClassifier(\n",
    "  (embedding_layer): BertEmbeddings(\n",
    "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "    (position_embeddings): Embedding(512, 768)\n",
    "    (token_type_embeddings): Embedding(2, 768)\n",
    "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "    (dropout): Dropout(p=0.1, inplace=False)\n",
    "  )\n",
    "  (lstm): LSTM(768, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
    "  (output_layer): Linear(in_features=128, out_features=2, bias=True)\n",
    "  (sigmoid): Sigmoid()\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings\n",
    "from transformers import AutoConfig\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_classes, num_layers, device, drop_prob=0.2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        config = AutoConfig.from_pretrained('./pretrained/bert-base-uncased')\n",
    "        self.embedding_layer = BertEmbeddings(config)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=True, dropout=drop_prob, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding_layer(input) # (batch_size, seq_length, embedding_size)\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden) # (batch_size, seq_length, hidden_size)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.output_layer(out) # (batch_size, seq_length, num_classes)\n",
    "        out = self.sigmoid(out)\n",
    "        out = out[:, -1, :]\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = (torch.zeros(self.n_layers * 2, batch_size, self.hidden_dim).to(self.device),\n",
    "                  torch.zeros(self.n_layers * 2, batch_size, self.hidden_dim).to(self.device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "A stratified 5-fold cross validation will be applied to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "def model_train(model, train_dataloader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        train_losses = []\n",
    "\n",
    "        model.train()\n",
    "        for i, batch_data in enumerate(train_dataloader):\n",
    "            input_ids, input_labels = tuple(data.to(device) for data in batch_data)\n",
    "            cur_batch = len(input_ids)\n",
    "            hidden = model.init_hidden(cur_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, hidden =  model(input_ids, hidden)\n",
    "\n",
    "            loss = criterion(output, input_labels)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epoch: {}/{}\".format((epoch + 1), epochs),\n",
    "              \"\\n\\tTraining Loss: {:.4f}\".format(np.mean(train_losses)))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# validation model\n",
    "def model_validation(model, val_dataloader, criterion):\n",
    "    val_losses = []\n",
    "    accuracy_score_list, recall_score_list, precision_score_list, f1_score_list = [], [], [], []\n",
    "\n",
    "    model.eval()\n",
    "    for batch_data in val_dataloader:\n",
    "        input_ids, input_labels = tuple(data.to(device) for data in batch_data)\n",
    "\n",
    "        cur_batch = len(input_ids)\n",
    "        hidden = model.init_hidden(cur_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds, hidden = model(input_ids, hidden)\n",
    "\n",
    "        loss = criterion(preds, input_labels)\n",
    "        val_losses.append(loss.item())\n",
    "        preds = preds.detach().to('cpu').numpy()\n",
    "        labels = input_labels.to('cpu').numpy()\n",
    "        preds = preds.argmax(1) # shape = [1, : ]\n",
    "\n",
    "        # Evaluate model\n",
    "        AccuracyScore = accuracy_score(labels, preds)\n",
    "        RecallScore = recall_score(labels, preds)\n",
    "        PrecisionScore = precision_score(labels, preds)\n",
    "        F1Score = f1_score(labels, preds)\n",
    "        \n",
    "        # Add to lists\n",
    "        accuracy_score_list.append(AccuracyScore)\n",
    "        recall_score_list.append(RecallScore)\n",
    "        precision_score_list.append(PrecisionScore)\n",
    "        f1_score_list.append(F1Score)\n",
    "\n",
    "    print(\"Validation Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "          \"Validation Accuracy: {:.4f}%\".format(np.mean(accuracy_score_list) * 100))\n",
    "    \n",
    "    return np.mean(accuracy_score_list), np.mean(recall_score_list), np.mean(precision_score_list), np.mean(f1_score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "embedding_dim = 768\n",
    "hidden_size = 64\n",
    "num_classes = 2\n",
    "num_layers = 2\n",
    "\n",
    "learning_rate = 0.0005\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1\n",
      "Epoch: 1/4 \n",
      "\tTraining Loss: 0.6166\n",
      "Epoch: 2/4 \n",
      "\tTraining Loss: 0.4079\n",
      "Epoch: 3/4 \n",
      "\tTraining Loss: 0.3781\n",
      "Epoch: 4/4 \n",
      "\tTraining Loss: 0.3712\n",
      "Validation Loss: 0.3670 Validation Accuracy: 94.4687%\n",
      "Time:  2\n",
      "Epoch: 1/4 \n",
      "\tTraining Loss: 0.6468\n",
      "Epoch: 2/4 \n",
      "\tTraining Loss: 0.3947\n",
      "Epoch: 3/4 \n",
      "\tTraining Loss: 0.3733\n",
      "Epoch: 4/4 \n",
      "\tTraining Loss: 0.3657\n",
      "Validation Loss: 0.3648 Validation Accuracy: 94.6759%\n",
      "Time:  3\n",
      "Epoch: 1/4 \n",
      "\tTraining Loss: 0.5343\n",
      "Epoch: 2/4 \n",
      "\tTraining Loss: 0.3812\n",
      "Epoch: 3/4 \n",
      "\tTraining Loss: 0.3688\n",
      "Epoch: 4/4 \n",
      "\tTraining Loss: 0.3621\n",
      "Validation Loss: 0.3597 Validation Accuracy: 95.1613%\n",
      "Time:  4\n",
      "Epoch: 1/4 \n",
      "\tTraining Loss: 0.5747\n",
      "Epoch: 2/4 \n",
      "\tTraining Loss: 0.4750\n",
      "Epoch: 3/4 \n",
      "\tTraining Loss: 0.4224\n",
      "Epoch: 4/4 \n",
      "\tTraining Loss: 0.3990\n",
      "Validation Loss: 0.3945 Validation Accuracy: 91.6469%\n",
      "Time:  5\n",
      "Epoch: 1/4 \n",
      "\tTraining Loss: 0.5471\n",
      "Epoch: 2/4 \n",
      "\tTraining Loss: 0.3797\n",
      "Epoch: 3/4 \n",
      "\tTraining Loss: 0.3683\n",
      "Epoch: 4/4 \n",
      "\tTraining Loss: 0.3626\n",
      "Validation Loss: 0.3642 Validation Accuracy: 94.7486%\n",
      "Accuracy: 94.14%\n",
      "Recall: 94.19%\n",
      "Precision: 94.13%\n",
      "F1_score: 93.96%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=60)\n",
    "accuracy_score_lists, recall_score_lists, precision_score_lists, f1_score_lists = [], [], [], []\n",
    "\n",
    "\n",
    "for time, (train_index, val_index) in enumerate(skfolds.split(sen_ids, sen_labels)):\n",
    "    print('Time: ', time + 1)\n",
    "    X_train, X_val = sen_ids[train_index], sen_ids[val_index]\n",
    "    Y_train, Y_val = sen_labels[train_index], sen_labels[val_index]\n",
    "\n",
    "    # pack dataloaders\n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    val_dataset = TensorDataset(X_val, Y_val)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = LSTMClassifier(embedding_dim, hidden_size, num_classes, num_layers, device)\n",
    "    model = model.cuda()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # train model\n",
    "    model = model_train(model, train_dataloader, criterion, optimizer, epochs)\n",
    "\n",
    "    # validate model\n",
    "    AccuracyScore, RecallScore, PrecisionScore, F1Score = model_validation(model, val_dataloader, criterion)\n",
    "\n",
    "    accuracy_score_lists.append(AccuracyScore)\n",
    "    recall_score_lists.append(RecallScore)\n",
    "    precision_score_lists.append(PrecisionScore)\n",
    "    f1_score_lists.append(F1Score)\n",
    "\n",
    "print(\"Accuracy: {:.2%}\".format(np.average(accuracy_score_lists)))\n",
    "print(\"Recall: {:.2%}\".format(np.average(recall_score_lists)))\n",
    "print(\"Precision: {:.2%}\".format(np.average(precision_score_lists)))\n",
    "print(\"F1_score: {:.2%}\".format(np.average(f1_score_lists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4 \n",
      "\tTraining Loss: 0.5142\n",
      "Epoch: 2/4 \n",
      "\tTraining Loss: 0.4064\n",
      "Epoch: 3/4 \n",
      "\tTraining Loss: 0.3852\n",
      "Epoch: 4/4 \n",
      "\tTraining Loss: 0.3825\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(sen_ids, sen_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = LSTMClassifier(embedding_dim, hidden_size, num_classes, num_layers, device)\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# train model\n",
    "model = model_train(model, train_dataloader, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_state_dict = model.state_dict()\n",
    "\n",
    "torch.save(net_state_dict, './model/LSTM_classifier.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to load the model:\n",
    "\n",
    "```python\n",
    "m_state_dict = torch.load('./model/LSTM_classifier.pt')\n",
    "model = LSTMClassifier(768, 64, 2, 2, device)\n",
    "model.load_state_dict(m_state_dict)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
