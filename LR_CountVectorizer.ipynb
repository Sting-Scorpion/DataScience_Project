{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing baseline model with `CountVectorizer()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "X = df['text']\n",
    "Y = df['suicide']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "### WordVectorization with `CountVectorizer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X)\n",
    "X_set = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (185659, 146321)\n",
      "Shape of Y: (185659,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X:\", X_set.shape)\n",
    "print(\"Shape of Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the baseline model\n",
    "\n",
    "For testing purposes, we will use the Logistic Regression classifier `LogisticRegression()` with its default settings as our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Baseline Model\n",
    "\n",
    "### 5-fold Stratified Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1\n",
      "\taccuracy_score: 0.9350\n",
      "\trecall_score: 0.9152\n",
      "\tprecision_score: 0.9531\n",
      "\tf1_score: 0.9338\n",
      "Time:  2\n",
      "\taccuracy_score: 0.9316\n",
      "\trecall_score: 0.9120\n",
      "\tprecision_score: 0.9492\n",
      "\tf1_score: 0.9303\n",
      "Time:  3\n",
      "\taccuracy_score: 0.9322\n",
      "\trecall_score: 0.9121\n",
      "\tprecision_score: 0.9505\n",
      "\tf1_score: 0.9309\n",
      "Time:  4\n",
      "\taccuracy_score: 0.9333\n",
      "\trecall_score: 0.9133\n",
      "\tprecision_score: 0.9515\n",
      "\tf1_score: 0.9320\n",
      "Time:  5\n",
      "\taccuracy_score: 0.9327\n",
      "\trecall_score: 0.9144\n",
      "\tprecision_score: 0.9492\n",
      "\tf1_score: 0.9315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=60)\n",
    "accuracy_score_list, recall_score_list, precision_score_list, f1_score_list = [], [], [], []\n",
    "\n",
    "for time, (train_index, test_index) in enumerate(skfolds.split(X_set, Y)):\n",
    "    X_train, X_test = X_set[train_index], X_set[test_index]\n",
    "    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "    clf.fit(X_train, Y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    AccuracyScore = accuracy_score(Y_test, y_pred)\n",
    "    RecallScore = recall_score(Y_test, y_pred)\n",
    "    PrecisionScore = precision_score(Y_test, y_pred)\n",
    "    F1Score = f1_score(Y_test, y_pred)\n",
    "\n",
    "    # Add to lists\n",
    "    accuracy_score_list.append(AccuracyScore)\n",
    "    recall_score_list.append(RecallScore)\n",
    "    precision_score_list.append(PrecisionScore)\n",
    "    f1_score_list.append(F1Score)\n",
    "\n",
    "    # Print the matrix\n",
    "    print('Time: ', time + 1)\n",
    "    print('\\taccuracy_score: {:.4f}'.format(AccuracyScore))\n",
    "    print('\\trecall_score: {:.4f}'.format(RecallScore))\n",
    "    print('\\tprecision_score: {:.4f}'.format(PrecisionScore))\n",
    "    print('\\tf1_score: {:.4f}'.format(F1Score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.30%\n",
      "Recall: 91.34%\n",
      "Precision: 95.07%\n",
      "F1_score: 93.17%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Print the average value of each matrix\n",
    "print(\"Accuracy: {:.2%}\".format(np.average(accuracy_score_list)))\n",
    "print(\"Recall: {:.2%}\".format(np.average(recall_score_list)))\n",
    "print(\"Precision: {:.2%}\".format(np.average(precision_score_list)))\n",
    "print(\"F1_score: {:.2%}\".format(np.average(f1_score_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2253]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "clf.fit(X_set, Y)\n",
    "\n",
    "print(clf.n_iter_)\n",
    "\n",
    "with open('./model/LR_count.pickle', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using code below to load the fitted model:\n",
    "\n",
    "``` python\n",
    "with open('./model/LR_count.pickle', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.14041675]\n",
      "[-0.19636139]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "print(clf.coef_[:, vocabulary['happy']])\n",
    "print(clf.coef_[:, vocabulary['sad']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights for class 0:\n",
      "\n",
      "suicidei with weight 3.3283044555491035\n",
      "sw with weight 3.3344068213180083\n",
      "upi with weight 3.354254375064318\n",
      "lifei with weight 3.7136275275338586\n",
      "myselfi with weight 3.7195201693589732\n",
      "iti with weight 3.787535432196934\n",
      "anymorei with weight 3.9350069893944677\n",
      "diei with weight 4.134013213528239\n",
      "helpi with weight 4.426595743027976\n",
      "mei with weight 4.700628407770826\n"
     ]
    }
   ],
   "source": [
    "n_feats_to_show = 10\n",
    "\n",
    "# Flip the index so that values are keys and keys are values:\n",
    "keys = vectorizer.vocabulary_.values()\n",
    "values = vectorizer.vocabulary_.keys()\n",
    "vocab_inverted = dict(zip(keys, values))\n",
    "\n",
    "for c, weights_c in enumerate(clf.coef_):\n",
    "    print(f'\\nWeights for class {c}:\\n')\n",
    "    strongest_idxs = np.argsort(weights_c)[-n_feats_to_show:]\n",
    "\n",
    "    for idx in strongest_idxs:\n",
    "        print(f'{vocab_inverted[idx]} with weight {weights_c[idx]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
